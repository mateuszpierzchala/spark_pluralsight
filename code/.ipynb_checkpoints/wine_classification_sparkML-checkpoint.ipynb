{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksperyment polegający na przwidywaniu szczepu winogron na podstawie danych o winie\n",
    "\n",
    "## Import sparka i wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/cb/d8ff49ba885e2c88b8cf2967edd84235ffa9ac301bffef657dfa5605a112/pyspark-2.3.2.tar.gz (211.9MB)\n",
      "Collecting py4j==0.10.7 (from pyspark)\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Running setup.py bdist_wheel for pyspark: started\n",
      "  Running setup.py bdist_wheel for pyspark: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\mateusz.pierzchala\\AppData\\Local\\pip\\Cache\\wheels\\be\\7d\\34\\cd3cfbc75d8b6b6ae0658e5425348560b86d187fe3e53832cc\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Predicting the grape variety from wine characteristics\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "rawData = spark.read \\\n",
    "            .format('csv')\\\n",
    "            .option('header','false') \\\n",
    "            .load('../datasets/wine.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+----+----+---+----+----+---+----+----+----+----+----+\n",
      "|_c0|  _c1| _c2| _c3| _c4|_c5| _c6| _c7|_c8| _c9|_c10|_c11|_c12|_c13|\n",
      "+---+-----+----+----+----+---+----+----+---+----+----+----+----+----+\n",
      "|  1|14.23|1.71|2.43|15.6|127| 2.8|3.06|.28|2.29|5.64|1.04|3.92|1065|\n",
      "|  1| 13.2|1.78|2.14|11.2|100|2.65|2.76|.26|1.28|4.38|1.05| 3.4|1050|\n",
      "|  1|13.16|2.36|2.67|18.6|101| 2.8|3.24| .3|2.81|5.68|1.03|3.17|1185|\n",
      "|  1|14.37|1.95| 2.5|16.8|113|3.85|3.49|.24|2.18| 7.8| .86|3.45|1480|\n",
      "|  1|13.24|2.59|2.87|  21|118| 2.8|2.69|.39|1.82|4.32|1.04|2.93| 735|\n",
      "+---+-----+----+----+----+---+----+----+---+----+----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przypisanie nazw kolumnom i zapisanie ich w zmiennej 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rawData.toDF('Label',\n",
    "                      'Alcohol',\n",
    "                      'MalicAcid',\n",
    "                      'Ash',\n",
    "                      'AshAlalinity',\n",
    "                      'Magnesium',\n",
    "                       'TotalPhenols',\n",
    "                       'Favanoids',\n",
    "                       'NonflavanoidPhenols',\n",
    "                       'Proanthocyanins',\n",
    "                       'ColorIntensity',\n",
    "                       'Hue',\n",
    "                       'OD',\n",
    "                       'Proline'\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Label: string, Alcohol: string, MalicAcid: string, Ash: string, AshAlalinity: string, Magnesium: string, TotalPhenols: string, Favanoids: string, NonflavanoidPhenols: string, Proanthocyanins: string, ColorIntensity: string, Hue: string, OD: string, Proline: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+---------+----+------------+---------+------------+---------+-------------------+---------------+--------------+----+----+-------+\n",
      "|Label|Alcohol|MalicAcid| Ash|AshAlalinity|Magnesium|TotalPhenols|Favanoids|NonflavanoidPhenols|Proanthocyanins|ColorIntensity| Hue|  OD|Proline|\n",
      "+-----+-------+---------+----+------------+---------+------------+---------+-------------------+---------------+--------------+----+----+-------+\n",
      "|    1|  14.23|     1.71|2.43|        15.6|      127|         2.8|     3.06|                .28|           2.29|          5.64|1.04|3.92|   1065|\n",
      "|    1|   13.2|     1.78|2.14|        11.2|      100|        2.65|     2.76|                .26|           1.28|          4.38|1.05| 3.4|   1050|\n",
      "|    1|  13.16|     2.36|2.67|        18.6|      101|         2.8|     3.24|                 .3|           2.81|          5.68|1.03|3.17|   1185|\n",
      "|    1|  14.37|     1.95| 2.5|        16.8|      113|        3.85|     3.49|                .24|           2.18|           7.8| .86|3.45|   1480|\n",
      "|    1|  13.24|     2.59|2.87|          21|      118|         2.8|     2.69|                .39|           1.82|          4.32|1.04|2.93|    735|\n",
      "+-----+-------+---------+----+------------+---------+------------+---------+-------------------+---------------+--------------+----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Na potrzeby SparkML reorganizujemy dane w wektory (wektoryzowanie???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "def vectorize(data):\n",
    "    return data.rdd.map(lambda r: [r[0], Vectors.dense(r[1:])]).toDF(['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|[14.23,1.71,2.43,...|\n",
      "|    1|[13.2,1.78,2.14,1...|\n",
      "|    1|[13.16,2.36,2.67,...|\n",
      "|    1|[14.37,1.95,2.5,1...|\n",
      "|    1|[13.24,2.59,2.87,...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizedData = vectorize(dataset)\n",
    "vectorizedData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label='1', features=DenseVector([14.23, 1.71, 2.43, 15.6, 127.0, 2.8, 3.06, 0.28, 2.29, 5.64, 1.04, 3.92, 1065.0])),\n",
       " Row(label='1', features=DenseVector([13.2, 1.78, 2.14, 11.2, 100.0, 2.65, 2.76, 0.26, 1.28, 4.38, 1.05, 3.4, 1050.0])),\n",
       " Row(label='1', features=DenseVector([13.16, 2.36, 2.67, 18.6, 101.0, 2.8, 3.24, 0.3, 2.81, 5.68, 1.03, 3.17, 1185.0])),\n",
       " Row(label='1', features=DenseVector([14.37, 1.95, 2.5, 16.8, 113.0, 3.85, 3.49, 0.24, 2.18, 7.8, 0.86, 3.45, 1480.0])),\n",
       " Row(label='1', features=DenseVector([13.24, 2.59, 2.87, 21.0, 118.0, 2.8, 2.69, 0.39, 1.82, 4.32, 1.04, 2.93, 735.0]))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizedData.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import funkcji StringIndexer używanej do reprezentowania danych kategorycznych w formie numerycznej (orginalnie zmienna 'label' mimo wartosci 1,2,3 jest typu string, zamieniamy ją na typ float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol = 'label',\n",
    "                             outputCol = 'indexedLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label='1', features=DenseVector([14.23, 1.71, 2.43, 15.6, 127.0, 2.8, 3.06, 0.28, 2.29, 5.64, 1.04, 3.92, 1065.0]), indexedLabel=1.0),\n",
       " Row(label='1', features=DenseVector([13.2, 1.78, 2.14, 11.2, 100.0, 2.65, 2.76, 0.26, 1.28, 4.38, 1.05, 3.4, 1050.0]), indexedLabel=1.0),\n",
       " Row(label='1', features=DenseVector([13.16, 2.36, 2.67, 18.6, 101.0, 2.8, 3.24, 0.3, 2.81, 5.68, 1.03, 3.17, 1185.0]), indexedLabel=1.0),\n",
       " Row(label='1', features=DenseVector([14.37, 1.95, 2.5, 16.8, 113.0, 3.85, 3.49, 0.24, 2.18, 7.8, 0.86, 3.45, 1480.0]), indexedLabel=1.0),\n",
       " Row(label='1', features=DenseVector([13.24, 2.59, 2.87, 21.0, 118.0, 2.8, 2.69, 0.39, 1.82, 4.32, 1.04, 2.93, 735.0]), indexedLabel=1.0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexedData = labelIndexer.fit(vectorizedData).transform(vectorizedData)\n",
    "indexedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: string, features: vector, indexedLabel: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    3|\n",
      "|    1|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexedData.select('label').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|indexedLabel|\n",
      "+------------+\n",
      "|         0.0|\n",
      "|         1.0|\n",
      "|         2.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexedData.select('indexedLabel').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budowa  modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = indexedData.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(\n",
    "    labelCol = 'indexedLabel',\n",
    "    featuresCol = 'features',\n",
    "    maxDepth = 4,\n",
    "    impurity = 'gini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dtree.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mateusz.pierzchala\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py\", line 105, in __del__\n",
      "    SparkContext._active_spark_context._gateway.detach(self._java_obj)\n",
      "AttributeError: 'MulticlassClassificationEvaluator' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------+--------------+-------------+----------+\n",
      "|label|            features|indexedLabel| rawPrediction|  probability|prediction|\n",
      "+-----+--------------------+------------+--------------+-------------+----------+\n",
      "|    1|[13.05,1.73,2.04,...|         1.0|[0.0,46.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|    1|[13.07,1.5,2.1,15...|         1.0|[0.0,46.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|    1|[13.24,3.98,2.29,...|         1.0|[47.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|    1|[13.5,1.81,2.61,2...|         1.0|[0.0,46.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|    1|[13.56,1.71,2.31,...|         1.0|[0.0,46.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "+-----+--------------------+------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'indexedLabel',\n",
    "                                             predictionCol = 'prediction',\n",
    "                                             metricName = 'f1')\n",
    "\n",
    "transformed_data = model.transform(testData)\n",
    "transformed_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 accuracy:  0.8604651162790697\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.getMetricName(),\n",
    "     'accuracy: ',\n",
    "     evaluator.evaluate(transformed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
